{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_setup",
        "outputId": "71534ca2-5884-4e62-aa50-dc9ad45ae070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading done\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load InLegalBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
        "model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(\"loading done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "chunk_function"
      },
      "outputs": [],
      "source": [
        "def chunk(text, max_length=512, stride=256):\n",
        "    \"\"\"\n",
        "    Split a long document into chunks of max_length tokens using InLegalBERT tokenizer with overlap.\n",
        "    Args:\n",
        "        text: Input text to be chunked.\n",
        "        max_length: Maximum number of tokens per chunk.\n",
        "        stride: Number of tokens to overlap between chunks.\n",
        "    Returns:\n",
        "        List of token ID lists, each representing a chunk.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_length - stride):\n",
        "        chunk = tokens[max(0, i - stride):i + max_length]\n",
        "        if len(chunk) > 0:  # Only add non-empty chunks\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "get_vector_function"
      },
      "outputs": [],
      "source": [
        "def get_vector(text):\n",
        "    \"\"\"\n",
        "    Generate [CLS] embeddings for each chunk of the input text using InLegalBERT.\n",
        "    Args:\n",
        "        text: Input text to generate embeddings for.\n",
        "    Returns:\n",
        "        Tensor of shape (num_chunks, 768) containing [CLS] embeddings for each chunk.\n",
        "    \"\"\"\n",
        "    chunks = chunk(text)\n",
        "    cls_vectors = []\n",
        "\n",
        "    for chunk_ids in chunks:\n",
        "        input_ids = torch.tensor([chunk_ids])  # shape: [1, seq_len]\n",
        "        attention_mask = torch.ones_like(input_ids)  # shape: [1, seq_len]\n",
        "\n",
        "        inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            cls_vector = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n",
        "            cls_vectors.append(cls_vector.squeeze(0))  # shape: [768]\n",
        "\n",
        "    # Stack all [CLS] vectors into a single tensor\n",
        "    doc_tensor = torch.stack(cls_vectors)  # shape: [num_chunks, 768]\n",
        "    print(\"Document tensor shape:\", doc_tensor.shape)\n",
        "    return doc_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace with your actual CSV file path\n",
        "csv_path = \"ILDC_multi/ILDC_multi.csv\"\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Assuming columns are named 'text' and 'label'\n",
        "train_texts = df['text'].tolist()\n",
        "train_labels = df['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "train_data",
        "outputId": "6c997dda-29c8-4fc3-a954-1f94b1d07fb4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m train_texts \u001b[38;5;241m=\u001b[39m train_split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     28\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train_split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 29\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(train_labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Extract texts and labels for dev\u001b[39;00m\n\u001b[0;32m     32\u001b[0m dev_texts \u001b[38;5;241m=\u001b[39m dev_split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "csv_path = \"ILDC_multi/ILDC_multi.csv\"\n",
        "\n",
        "# Load the CSV file, including the 'split' column\n",
        "dtype = {'text': str, 'label': int, 'split': str}\n",
        "df = pd.read_csv(csv_path, usecols=['text', 'label', 'split', 'name'], low_memory=False, dtype=dtype)\n",
        "6\n",
        "# Use only a random subset (e.g., 1000 rows)\n",
        "df = df.sample(n=50, random_state=42)\n",
        "\n",
        "# Filter for the train split\n",
        "train_df = df[df['split'] == 'train']\n",
        "test_df = df[df['split'] == 'test']\n",
        "\n",
        "\n",
        "# Split train_df into train and dev (e.g., 80% train, 20% dev)\n",
        "train_split_df, dev_split_df = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.2,      # 20% for dev\n",
        "    random_state=42,    # for reproducibility\n",
        "    stratify=train_df['label']  # stratify to keep label distribution\n",
        ")\n",
        "\n",
        "# Extract texts and labels for training\n",
        "train_texts = train_split_df['text'].tolist()\n",
        "train_labels = train_split_df['label'].tolist()\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n",
        "# Extract texts and labels for dev\n",
        "dev_texts = dev_split_df['text'].tolist()\n",
        "dev_labels = dev_split_df['label'].tolist()\n",
        "dev_labels = torch.tensor(dev_labels, dtype=torch.long)\n",
        "\n",
        "# Extract texts and labels for testing\n",
        "test_texts = test_df['text'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_vectors = []\n",
        "for item in train_texts:\n",
        "    doc_tensor = get_vector(item)        # shape = [num_chunks, 768]\n",
        "    pooled     = doc_tensor.mean(0)      # shape = [768]  ← NEW\n",
        "    train_vectors.append(pooled)\n",
        "\n",
        "# Stack vectors: shape (num_docs, num_chunks, 768)\n",
        "X_train = torch.stack(train_vectors)  # shape: [4, num_chunks, 768]\n",
        "\n",
        "# Save to workspace\n",
        "file_path = \"data_train.pt\"\n",
        "torch.save({'X': X_train, 'y': train_labels}, file_path)\n",
        "print(\"Saved training data:\", file_path)\n",
        "\n",
        "\n",
        "test_vectors = []\n",
        "for item in test_texts:\n",
        "    doc_tensor = get_vector(item)        # shape = [num_chunks, 768]\n",
        "    pooled     = doc_tensor.mean(0)      # shape = [768]  ← NEW\n",
        "    test_vectors.append(pooled)\n",
        "\n",
        "# Stack vectors: shape (num_docs, num_chunks, 768)\n",
        "X_test = torch.stack(test_vectors)  # shape: [3, num_chunks, 768]\n",
        "\n",
        "# Save to workspace\n",
        "file_path = \"data_test.pt\"\n",
        "torch.save({'X': X_train, 'y': train_labels}, file_path)\n",
        "print(\"Saved testing data:\", file_path)\n",
        "\n",
        "dev_vectors = []\n",
        "for item in dev_texts:\n",
        "    doc_tensor = get_vector(item)        # shape = [num_chunks, 768]\n",
        "    pooled     = doc_tensor.mean(0)      # shape = [768]  ← NEW\n",
        "    dev_vectors.append(pooled)\n",
        "\n",
        "# Stack vectors: shape (num_docs, num_chunks, 768)\n",
        "X_dev = torch.stack(dev_vectors)  # shape: [3, num_chunks, 768]\n",
        "\n",
        "# Save to workspace\n",
        "file_path = \"data_dev.pt\"\n",
        "torch.save({'X': X_dev, 'y': dev_labels}, file_path)\n",
        "print(\"Saved dev data:\", file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "ml-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
